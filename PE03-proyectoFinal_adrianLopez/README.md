# PE03 - Cluster Kubernetes con k3s

## Proyecto Final | OpciÃ³n D (Avanzado)

**Alumno:** AdriÃ¡n LÃ³pez  
**Asignatura:** VirtualizaciÃ³n y Cloud (CENY)  
**Curso:** ASIR2 - 2025/2026  
**Unidad:** UT6  

---

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa un **cluster Kubernetes de alta disponibilidad** usando **k3s** (versiÃ³n ligera de Kubernetes) con un nodo master y dos nodos worker. La infraestructura se despliega automÃ¡ticamente con **Vagrant** y se provisiona con **Ansible**.

Dentro del cluster se despliega una aplicaciÃ³n web accesible desde el navegador, un panel de monitorizaciÃ³n y certificados SSL autofirmados.

---

## ğŸ—ï¸ Diagrama de Arquitectura

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Host (tu ordenador)    â”‚
                    â”‚                             â”‚
                    â”‚  localhost:8080  â†’ App HTTP â”‚
                    â”‚  localhost:8443  â†’ App HTTPSâ”‚
                    â”‚  localhost:9090  â†’ Monitor  â”‚
                    â”‚  localhost:6443  â†’ API K8s  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”
                    â”‚     Master (192.168.56.10)    â”‚
                    â”‚     k3s server + kubectl      â”‚
                    â”‚     RAM: 2048 MB | 2 CPUs     â”‚
                    â”‚                               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚   Kubernetes API Server  â”‚ â”‚
                    â”‚  â”‚   Control Plane          â”‚ â”‚
                    â”‚  â”‚   Ansible Controller     â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€-â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Worker 1    â”‚  â”‚    Worker 2    â”‚
                â”‚ 192.168.56.11  â”‚  â”‚ 192.168.56.12  â”‚
                â”‚ k3s agent      â”‚  â”‚ k3s agent      â”‚
                â”‚ RAM: 1024 MB   â”‚  â”‚ RAM: 1024 MB   â”‚
                â”‚                â”‚  â”‚                â”‚
                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                â”‚  â”‚ Pod:     â”‚  â”‚  â”‚  â”‚ Pod:     â”‚  â”‚
                â”‚  â”‚ webapp   â”‚  â”‚  â”‚  â”‚ webapp   â”‚  â”‚
                â”‚  â”‚ (nginx)  â”‚  â”‚  â”‚  â”‚ (nginx)  â”‚  â”‚
                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                â”‚  â”‚ Pod:     â”‚  â”‚  â”‚  â”‚ Pod:     â”‚  â”‚
                â”‚  â”‚ monitor  â”‚  â”‚  â”‚  â”‚ webapp   â”‚  â”‚
                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estructura del Proyecto

```
PE03-proyectoFinal_adrianLopez/
â”œâ”€â”€ README.md                       # Este archivo
â”œâ”€â”€ Vagrantfile                     # ConfiguraciÃ³n de las 3 VMs
â”œâ”€â”€ config.yaml                     # Variables externas (+0.5 pts)
â”œâ”€â”€ scripts/                        # Scripts de shell
â”‚   â”œâ”€â”€ common.sh                   # ConfiguraciÃ³n comÃºn (hosts, swap, kernel)
â”‚   â”œâ”€â”€ install-ansible.sh          # InstalaciÃ³n de Ansible en master
â”‚   â”œâ”€â”€ run-ansible.sh              # Ejecuta los playbooks
â”‚   â””â”€â”€ health-check.sh             # Script de health-check (+0.5 pts)
â”œâ”€â”€ ansible/                        # Provisioning con Ansible (+1.0 pts)
â”‚   â”œâ”€â”€ site.yml                    # Playbook principal (orquestador)
â”‚   â””â”€â”€ roles/
â”‚       â”œâ”€â”€ k3s-master/             # InstalaciÃ³n k3s server
â”‚       â”‚   â”œâ”€â”€ tasks/main.yml
â”‚       â”‚   â””â”€â”€ defaults/main.yml
â”‚       â”œâ”€â”€ k3s-worker/             # InstalaciÃ³n k3s agent
â”‚       â”‚   â”œâ”€â”€ tasks/main.yml
â”‚       â”‚   â””â”€â”€ defaults/main.yml
â”‚       â”œâ”€â”€ ssl-certs/              # Certificados SSL (+0.5 pts)
â”‚       â”‚   â”œâ”€â”€ tasks/main.yml
â”‚       â”‚   â””â”€â”€ defaults/main.yml
â”‚       â”œâ”€â”€ k8s-app/                # Despliegue de la aplicaciÃ³n
â”‚       â”‚   â””â”€â”€ tasks/main.yml
â”‚       â””â”€â”€ monitoring/             # MonitorizaciÃ³n (+0.5 pts)
â”‚           â””â”€â”€ tasks/main.yml
â””â”€â”€ k8s/                            # Manifiestos de Kubernetes
    â”œâ”€â”€ namespace.yml               # Namespace
    â”œâ”€â”€ configmap-webapp.yml        # HTML + nginx config de la app
    â”œâ”€â”€ deployment-webapp.yml       # Deployment (3 rÃ©plicas)
    â”œâ”€â”€ service-webapp.yml          # Service HTTP (NodePort 30080)
    â”œâ”€â”€ service-webapp-https.yml    # Service HTTPS (NodePort 30443)
    â”œâ”€â”€ monitoring-configmap.yml    # Dashboard de monitorizaciÃ³n
    â”œâ”€â”€ monitoring-deployment.yml   # Deployment del monitor
    â””â”€â”€ monitoring-service.yml      # Service del monitor (NodePort 30090)
```

---

## ğŸ–¥ï¸ MÃ¡quinas Virtuales

| VM | Hostname | IP | RAM | CPUs | FunciÃ³n |
|---|---|---|---|---|---|
| Master | `master` | 192.168.56.10 | 2048 MB | 2 | Nodo master k3s + API Server + Ansible |
| Worker 1 | `worker1` | 192.168.56.11 | 1024 MB | 1 | Nodo worker k3s (ejecuta pods) |
| Worker 2 | `worker2` | 192.168.56.12 | 1024 MB | 1 | Nodo worker k3s (redundancia) |

**RAM total necesaria:** ~4 GB (recomendado 8-16 GB en el host)

---

## ğŸŒ Puertos y Servicios

| Servicio | Puerto Host | Puerto VM | DescripciÃ³n |
|---|---|---|---|
| App Web HTTP | `localhost:8080` | 30080 | AplicaciÃ³n web principal |
| App Web HTTPS | `localhost:8443` | 30443 | AplicaciÃ³n con SSL |
| Monitor | `localhost:9090` | 30090 | Dashboard de monitorizaciÃ³n |
| K8s API | `localhost:6443` | 6443 | API Server de Kubernetes |

---

## ğŸš€ Instrucciones de Uso

### Requisitos Previos
- **VirtualBox** 6.1+ instalado
- **Vagrant** 2.3+ instalado
- MÃ­nimo **8 GB de RAM** disponible
- ConexiÃ³n a Internet (para descargar k3s y la imagen de nginx)

### Despliegue Completo

```bash
# 1. Clonar/descargar el proyecto
cd PE03-proyectoFinal_adrianLopez

# 2. Levantar toda la infraestructura (automÃ¡tico)
vagrant up

# 3. Esperar ~10-15 minutos a que se complete el provisioning
# Todo se configura automÃ¡ticamente: k3s, Ansible, SSL, app, monitor

# 4. Verificar que todo funciona
vagrant status
```

### Acceso a los Servicios

```bash
# AplicaciÃ³n web
curl http://localhost:8080

# AplicaciÃ³n web con HTTPS (certificado autofirmado)
curl -k https://localhost:8443

# Dashboard de monitorizaciÃ³n
curl http://localhost:9090
```

### Comandos de Kubernetes (desde el master)

```bash
# Conectarse al master
vagrant ssh master

# Ver nodos del cluster
kubectl get nodes

# Ver pods y en quÃ© worker estÃ¡n
kubectl get pods -o wide

# Ver servicios
kubectl get svc

# Ver todos los recursos
kubectl get all

# Ejecutar health-check manual
./health-check.sh
```

---

## ğŸ† Puntos Extra Implementados

| Extra | Puntos | ImplementaciÃ³n |
|---|---|---|
| **Ansible** | +1.0 | Provisioning completo con Ansible (5 roles: k3s-master, k3s-worker, ssl-certs, k8s-app, monitoring) |
| **Variables externas** | +0.5 | Archivo `config.yaml` con toda la configuraciÃ³n parametrizada |
| **SSL/HTTPS** | +0.5 | Certificados autofirmados generados con OpenSSL, Secret TLS en K8s |
| **MonitorizaciÃ³n** | +0.5 | Dashboard web + script health-check.sh con cron cada 5 min |
| **README excelente** | +0.5 | Este README con diagrama, instrucciones completas y troubleshooting |

**Total puntos extra: +3.0**

---

## ğŸ”§ Troubleshooting

### Las VMs no arrancan
```bash
# Verificar que VirtualBox estÃ¡ instalado
VBoxManage --version

# Verificar que Vagrant estÃ¡ instalado
vagrant --version

# Destruir y recrear si hay problemas
vagrant destroy -f
vagrant up
```

### k3s no se instala correctamente
```bash
# Ver logs del master
vagrant ssh master
sudo journalctl -u k3s -f

# Ver logs de un worker
vagrant ssh worker1
sudo journalctl -u k3s-agent -f
```

### Los pods no arrancan
```bash
vagrant ssh master

# Ver estado de los pods
kubectl get pods -o wide

# Ver eventos
kubectl get events --sort-by='.lastTimestamp'

# Ver logs de un pod especÃ­fico
kubectl logs <nombre-del-pod>

# Describir un pod para ver errores
kubectl describe pod <nombre-del-pod>
```

### No puedo acceder a la app desde el host
```bash
# Verificar que el servicio estÃ¡ creado
vagrant ssh master
kubectl get svc

# Verificar que los pods estÃ¡n Running
kubectl get pods

# Probar desde dentro del master
curl http://localhost:30080
```

### Problemas de RAM
```bash
# Si tu PC tiene poca RAM, modifica config.yaml:
# - Reduce master a 1536 MB
# - Reduce workers a 768 MB

# Verificar uso de RAM en VirtualBox
VBoxManage list runningvms
```

### Resetear el cluster completo
```bash
vagrant destroy -f
vagrant up
```

---

## ï¿½ Incidencias y ResoluciÃ³n

Durante el desarrollo del proyecto se encontraron varias incidencias que requirieron investigaciÃ³n y correcciÃ³n. Se documentan aquÃ­ como referencia.

### Incidencia 1: Workers no se unen al cluster

**SÃ­ntoma:** Al ejecutar `kubectl get nodes` en el master, solo aparecÃ­a el nodo master. Los workers no se unÃ­an al cluster.

**Causa:** El token de k3s se leÃ­a en el rol del master pero no se propagaba correctamente a los workers. El rol `k3s-worker` usaba `delegate_to: localhost` y el mÃ³dulo `slurp` para leer el token del master, un patrÃ³n frÃ¡gil que fallaba silenciosamente.

**SoluciÃ³n:** Se reestructurÃ³ la comunicaciÃ³n del token entre roles:
1. En el rol `k3s-master`, tras leer el token, se guarda como fact con `set_fact: master_k3s_token`
2. En `site.yml`, el play de los workers recibe el token via `hostvars`: 
   ```yaml
   vars:
     k3s_token: "{{ hostvars[groups['master'][0]]['master_k3s_token'] }}"
   ```
3. Se aÃ±adiÃ³ un play intermedio que espera a que los 3 nodos estÃ©n en estado Ready antes de desplegar la aplicaciÃ³n

### Incidencia 2: SSH deniega la conexiÃ³n por contraseÃ±a a los workers

**SÃ­ntoma:** Ansible no podÃ­a conectar con los workers. El error era:
```
fatal: [192.168.56.11]: UNREACHABLE! => Permission denied (publickey)
```

**Causa:** La box `ubuntu/focal64` es una cloud image que incluye el fichero `/etc/ssh/sshd_config.d/60-cloudimg-settings.conf` con `PasswordAuthentication no`. Este fichero se carga con `Include /etc/ssh/sshd_config.d/*.conf` al inicio de `sshd_config` y tiene prioridad sobre cualquier cambio que se haga en el fichero principal.

**SoluciÃ³n:**
1. En `common.sh`, se comenta la directiva `PasswordAuthentication no` en todos los archivos de `/etc/ssh/sshd_config.d/`
2. Se crea un fichero propio `99-allow-password.conf` con mÃ¡xima prioridad que habilita la autenticaciÃ³n por contraseÃ±a
3. En el inventario de Ansible, se aÃ±adiÃ³ `ansible_ssh_common_args='-o PubkeyAuthentication=no'` a los workers para forzar la autenticaciÃ³n por contraseÃ±a

### Incidencia 3: Red entre nodos rota (HTTPS y Monitoring inaccesibles)

**SÃ­ntoma:** `http://localhost:8080` funcionaba pero `https://localhost:8443` daba `ERR_SSL_PROTOCOL_ERROR` y `http://localhost:9090` no cargaba. Desde el master, los pods en los workers no eran accesibles.

**Causa:** Flannel (la red overlay de k3s) usaba la interfaz NAT de VirtualBox (`enp0s3` con IP `10.0.2.15`) en lugar de la red privada (`enp0s8` con IPs `192.168.56.x`). Como todas las VMs comparten la misma IP NAT, el trÃ¡fico entre nodos nunca llegaba. HTTP funcionaba "por suerte" porque una rÃ©plica del pod existÃ­a en el propio master.

**SoluciÃ³n:** Se aÃ±adiÃ³ el flag `--flannel-iface enp0s8` tanto en la instalaciÃ³n del servidor k3s (master) como en los agentes (workers), forzando a Flannel a usar la red privada de VirtualBox.

### Incidencia 4: HTTPS devuelve error SSL

**SÃ­ntoma:** `curl -k https://localhost:30443` devolvÃ­a `error:1408F10B:SSL routines:ssl3_get_record:wrong version number`.

**Causa:** El servicio `webapp-https` enviaba el trÃ¡fico al `targetPort: 80` del contenedor nginx, que solo servÃ­a HTTP plano. El cliente esperaba una conexiÃ³n TLS pero recibÃ­a HTTP.

**SoluciÃ³n:**
1. Se aÃ±adiÃ³ un bloque `server` SSL en la configuraciÃ³n de nginx (configmap) que escucha en el puerto 443 con los certificados TLS
2. Se montÃ³ el Secret `webapp-tls` como volumen en el Deployment para que nginx tenga acceso a los certificados
3. Se corrigiÃ³ el `targetPort` del servicio HTTPS de 80 a 443

---

## ï¿½ğŸ“š TecnologÃ­as Utilizadas

- **Vagrant** - GestiÃ³n de mÃ¡quinas virtuales
- **VirtualBox** - Hipervisor
- **Ubuntu 20.04 (Focal)** - Sistema operativo de las VMs
- **k3s** - DistribuciÃ³n ligera de Kubernetes
- **Ansible** - AutomatizaciÃ³n del provisioning
- **Kubernetes** - OrquestaciÃ³n de contenedores
- **Nginx** - Servidor web (contenedor)
- **OpenSSL** - GeneraciÃ³n de certificados SSL

---

## ğŸ“ Notas

- El proyecto estÃ¡ diseÃ±ado para funcionar con un solo `vagrant up`
- Todos los scripts son idempotentes (se pueden ejecutar mÃºltiples veces sin errores)
- El health-check se ejecuta automÃ¡ticamente cada 5 minutos vÃ­a cron
- Los certificados SSL son autofirmados (el navegador mostrarÃ¡ una advertencia de seguridad)

---

## Autor

- Nombre: **AdriÃ¡n LÃ³pez**
- LinkedIn: https://www.linkedin.com/in/adriÃ¡n-lÃ³pez-10b7b2398
- Ciclo: **AdministraciÃ³n de Sistemas InformÃ¡ticos en Red (2Âº ASIR)**
- Video del proyecto: https://youtu.be/wXAQCQ9v_f8 
